.TH scrape_pipeline 1 "September 2023" "scrape_pipeline" "User Commands"
.SH NAME
scrape_pipeline \- collect usernames and emails for given niches using public scraping tools
.SH SYNOPSIS
.B scrape_pipeline
.RI [ options ]
.SH DESCRIPTION
.B scrape_pipeline
is a command-line helper that runs previewable scraping stubs for Reddit, Quora, Twitter/X, Amazon, YouTube, and forums.
It collects publicly visible usernames or email addresses related to target keywords, writes JSONL/CSV outputs per source, and stores deduplicated contacts in SQLite.
.PP
The pipeline is stubbed for demonstration and is intended for educational, lawful use only. Replace source fetchers with your own compliant scrapers.
.SH OPTIONS
.TP
.BR --keywords " " "<kw1> [<kw2> ...]"
Keywords or niches to target (default: "weight loss" "cybersecurity" "passive income").
.TP
.BR --limit " " <int>
Maximum items per source (default: 25).
.TP
.BR --output-dir " " <path>
Directory for per-source JSONL/CSV outputs (default: data/latest).
.TP
.BR --db-path " " <path>
Path to SQLite database for deduplicated contacts (default: data/contacts.db).
.TP
.BR --proxy " " <url>
Proxy URL such as http://host:port. Repeat to supply multiple proxies.
.TP
.BR --proxy-file " " <path>
File containing one proxy URL per line to rotate through during scraping.
.TP
.BR --preview
Preview mode; skips live scraping and shows placeholder results.
.TP
.B -h , --help
Show command summary and exit.
.SH EXAMPLES
.TP
Run with defaults and preview mode disabled
.B scrape_pipeline --keywords "ai tools" "bug bounties" --limit 10
.TP
Preview planned calls without scraping
.B scrape_pipeline --preview
.TP
Load proxies from a file
.B scrape_pipeline --proxy-file proxies.txt --limit 5
.SH FILES
.TP
.I data/latest
Default output directory for JSONL/CSV files.
.TP
.I data/contacts.db
Default SQLite database path.
.SH SEE ALSO
.B python\ -m\ scrape_pipeline
runs the same command via the Python module entrypoint.
.SH AUTHOR
Generated by GPT-5.1-Codex-Max for demonstration purposes.
